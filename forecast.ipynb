{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import TensorBoard, History\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from evaluation import *\n",
    "from machineLearningModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)\n",
    "\n",
    "## net params\n",
    "num_layers = 1#3#6\n",
    "num_neurons = 300#500\n",
    "batch_size = 100#500#1000\n",
    "dropout_rate = 0\n",
    "#const_features = ['latitude', 'longitude', 'altitude', 'modules_per_string', 'strings_per_inverter', 'tilt',\n",
    "#                  'azimuth', 'albedo', 'Technology', 'BIPV', 'A_c', 'N_s', 'pdc0', 'gamma_pdc', 'SystemID']#15\n",
    "#dyn_features = ['Wind Direction_x', 'Wind Direction_y', 'Total Cloud Cover', 'Low Cloud Cover', 'Medium Cloud Cover',\n",
    "#                'High Cloud Cover', 'Wind Speed', 'Wind Gust', 'Total Precipitation',\n",
    "#                'Snow Fraction', 'Mean Sea Level Pressure', 'DIF - backwards', 'DNI - backwards', 'Shortwave Radiation',\n",
    "#                'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']#20\n",
    "const_features = ['SystemID']\n",
    "dyn_features = ['DIF - backwards', 'DNI - backwards', 'Shortwave Radiation', 'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']\n",
    "target_features = ['power']\n",
    "drop_features = ['power_pvlib']\n",
    "act_fct = 'relu'\n",
    "out_act = 'linear'#'relu'\n",
    "loss_fct = 'mae'\n",
    "optim = 'adam'\n",
    "metrics = []\n",
    "history = History()\n",
    "val_history = History()\n",
    "\n",
    "## data params\n",
    "filename = './data/full_data_5_systems.csv'\n",
    "correlations = ['pearson']#'pearson', 'spearman', 'kendall']\n",
    "timesteps = 5#24\n",
    "shape = (len(const_features) + len(dyn_features) + timesteps * (len(dyn_features) + len(target_features)),)# - 1\n",
    "\n",
    "## training params\n",
    "tensorboard = False\n",
    "shuffle = True\n",
    "epochs = 20\n",
    "val_split = 1.0/10.0\n",
    "forecast_horizon = 3#24\n",
    "sliding_window = 24#168#24\n",
    "dir = './test_results/'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_results/preprocessed_data_t-5_f60.csv\n",
      "Loading preprocessed dataset ...\n"
     ]
    }
   ],
   "source": [
    "pfname = dir + 'preprocessed_data_t-'+str(timesteps)+'_f'+str(shape[0])+'.csv'\n",
    "print(pfname)\n",
    "prep = Path(pfname)\n",
    "if prep.exists():\n",
    "    print('Loading preprocessed dataset ...')\n",
    "    pvlib = np.array_split(pd.read_csv(filename, skipinitialspace=True).set_index('time'), 5)[-1].power_pvlib\n",
    "    dataset = pd.read_csv(pfname, skipinitialspace=True).set_index(['time', 'SystemID'])\n",
    "else:\n",
    "    print('Data preprocessing ...')\n",
    "    df = pd.read_csv(filename, skipinitialspace=True).set_index('time')\n",
    "    df = np.array_split(df, 5)[-1] ##################################\n",
    "    pvlib = df.power_pvlib\n",
    "    dataset = df[const_features + dyn_features + target_features].copy()[:'2017-02-09 10:00:00']\n",
    "\n",
    "    #separate system\n",
    "    for i in range(1, timesteps + 1):\n",
    "        for feature in dyn_features + target_features:\n",
    "            sys.stdout.write(\"Shifting %i/%i %s                \\r\" % (i, timesteps, feature))\n",
    "            sys.stdout.flush()\n",
    "            dataset[feature + ' t-' + str(i)] = dataset.shift(i)[feature]\n",
    "    print('Shifting done.                ')\n",
    "\n",
    "    dataset['forecast_horizon'] = 0\n",
    "    p = dataset[target_features]\n",
    "    dataset = dataset.drop(target_features, axis=1)\n",
    "    for f in target_features:\n",
    "        dataset[f] = p[f]\n",
    "    dataset = dataset.dropna().reset_index().set_index(['time', 'SystemID'])\n",
    "\n",
    "    sys.stdout.write(\"Writing to file ...\\r\")\n",
    "    sys.stdout.flush()\n",
    "    dataset.to_csv(pfname, encoding='utf-8')\n",
    "    print('Writing done.                ')\n",
    "\n",
    "    if correlations:\n",
    "        sys.stdout.write('Computing correlations ...\\r')\n",
    "        sys.stdout.flush()\n",
    "        for corr in correlations:\n",
    "            sys.stdout.write(\"Computing %s correlation matrix                \\r\" % (corr))\n",
    "            sys.stdout.flush()\n",
    "            dataset.corr(method=corr).to_csv(dir + corr + '_correlations.csv', encoding='utf-8')\n",
    "        print('Correlations done.                   ')\n",
    "\n",
    "train, test = dataset[:('2015-10-12 06:00:00', 4.0)], dataset[('2015-10-12 07:00:00', 4.0):]\n",
    "trainX, trainY = train.iloc[:,:-len(target_features)], train.iloc[:,-len(target_features):]\n",
    "testX, testY = test.iloc[:,:-len(target_features)], test.iloc[:,-len(target_features):]\n",
    "idx = testX.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MultiLayerPerceptron\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tensorboard:\n",
    "    print('tensorboard activated')\n",
    "    callbacks = [TensorBoard(log_dir='./tensorboard', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False), history]\n",
    "else:\n",
    "    callbacks = [history]\n",
    "\n",
    "#model = SARIMAX((0,1,2), (1,0,0,24))\n",
    "model = MultiLayerPerceptron(shape, len(target_features), num_layers, num_neurons, loss_fct, optim,\n",
    "                 act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                 epochs, val_split, callbacks, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61367 samples, validate on 6819 samples\n",
      "Epoch 1/20\n",
      "61367/61367 [==============================] - 2s 25us/step - loss: 223.0276 - val_loss: 143.5808\n",
      "Epoch 2/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 127.3470 - val_loss: 114.3382\n",
      "Epoch 3/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 105.6715 - val_loss: 111.7478\n",
      "Epoch 4/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 89.4181 - val_loss: 87.2424\n",
      "Epoch 5/20\n",
      "61367/61367 [==============================] - 1s 20us/step - loss: 78.7551 - val_loss: 89.1138\n",
      "Epoch 6/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 74.3667 - val_loss: 86.7696\n",
      "Epoch 7/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 69.7839 - val_loss: 75.6664\n",
      "Epoch 8/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 66.5295 - val_loss: 70.5067\n",
      "Epoch 9/20\n",
      "61367/61367 [==============================] - 1s 20us/step - loss: 62.7812 - val_loss: 64.5778\n",
      "Epoch 10/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 64.0003 - val_loss: 66.4398\n",
      "Epoch 11/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 59.1903 - val_loss: 63.2300\n",
      "Epoch 12/20\n",
      "61367/61367 [==============================] - 1s 20us/step - loss: 59.1381 - val_loss: 74.3242\n",
      "Epoch 13/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 58.2419 - val_loss: 67.2356\n",
      "Epoch 14/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 56.3671 - val_loss: 88.0977\n",
      "Epoch 15/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 55.5498 - val_loss: 62.0693\n",
      "Epoch 16/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 55.6562 - val_loss: 59.4984\n",
      "Epoch 17/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 54.4671 - val_loss: 65.2124\n",
      "Epoch 18/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 54.9942 - val_loss: 54.9365\n",
      "Epoch 19/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 52.2805 - val_loss: 58.0269\n",
      "Epoch 20/20\n",
      "61367/61367 [==============================] - 1s 21us/step - loss: 51.8900 - val_loss: 59.4790\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "X = trainX\n",
    "y = trainY\n",
    "if shuffle:\n",
    "    df = pd.DataFrame(np.concatenate((trainX, trainY), axis=1))\n",
    "    df = df.sample(frac=1).values\n",
    "    y = df[:, -len(target_features):]\n",
    "    X = df[:, :-len(target_features)]\n",
    "\n",
    "model.learn(X, y)\n",
    "\n",
    "name = './saved_models/pretrained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)+'_sys'+str(4)\n",
    "# serialize model to JSON\n",
    "model_json = model.model.to_json()\n",
    "with open(name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.model.save_weights(name + \".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk-Forward Validation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk-Forward Validation 11664/11664\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "method = 'pvlib'\n",
    "\n",
    "model.epochs = 5\n",
    "model.validation_split = 0.0\n",
    "model.batch_size = sliding_window\n",
    "model.verbose = 0\n",
    "model.callbacks = [val_history]\n",
    "\n",
    "predictions = []\n",
    "length = len(testX) - forecast_horizon - 1     #-11000\n",
    "for i in range(length):# - 10\n",
    "    sys.stdout.write(\"Walk-Forward Validation %i/%i\\r\" % (i+1, length))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # initialize values for lagged power columns\n",
    "    p = []\n",
    "    for l in range(1, timesteps + 1):\n",
    "        p.append(testX.iloc[i:i+1,:]['power t-'+str(l)].values[0])\n",
    "        \n",
    "    ps = []\n",
    "    ts = []\n",
    "    for f in range(forecast_horizon):\n",
    "        # build input vector for future timestep\n",
    "        t = testX.iloc[i+f:i+1+f,:].copy()\n",
    "        for l in range(timesteps-1, 1, -1):\n",
    "            t['power t-' + str(l+1)] = p[l]\n",
    "            p[l] = p[l-1]\n",
    "        t['power t-1'] = p[0]\n",
    "        t['forecast_horizon'] = f\n",
    "        ts.append(t)\n",
    "        \n",
    "        # make prediction for input new vector\n",
    "        p[0] = model.forecast(t).item(0)\n",
    "        ps.append(p[0])\n",
    "        \n",
    "    for f in range(forecast_horizon): # to avoid possible information leakage?\n",
    "        # train with newly available data\n",
    "        model.learn(ts[f], testY.iloc[i+f:i+1+f,:])\n",
    "        \n",
    "    predictions.append(pd.DataFrame(ps))\n",
    "    \n",
    "prediction = pd.concat(predictions)\n",
    "\n",
    "name = './saved_models/trained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)+'_sys'+str(4)\n",
    "# serialize model to JSON\n",
    "model_json = model.model.to_json()\n",
    "with open(name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.model.save_weights(name + \".h5\")\n",
    "print(\"\\nSaved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily mean +1h-prediction RMSE: 400.94355784576646\n",
      "daily mean pvlib forecast RMSE: 1319.6280018979849\n",
      "                 0\n",
      "count   485.000000\n",
      "mean    400.943558\n",
      "std     273.097217\n",
      "min      13.796312\n",
      "25%     202.228216\n",
      "50%     348.250759\n",
      "75%     562.313674\n",
      "max    1779.027531\n",
      "                 0\n",
      "count   485.000000\n",
      "mean   1319.628002\n",
      "std     697.310573\n",
      "min       0.000000\n",
      "25%     800.629210\n",
      "50%    1191.894250\n",
      "75%    1697.159243\n",
      "max    3500.072065\n",
      "+1h-prediction test RMSE: 484.829\n",
      "pvlib forecast test RMSE: 1492.981\n",
      "+1h-prediction nice day RMSE: 228.829\n",
      "pvlib forecast nice day RMSE: 444.823\n",
      "daily mean +2h-prediction RMSE: 560.1512879890648\n",
      "daily mean pvlib forecast RMSE: 1319.6280018979849\n",
      "                 0\n",
      "count   485.000000\n",
      "mean    560.151288\n",
      "std     357.983675\n",
      "min      10.553615\n",
      "25%     266.679527\n",
      "50%     534.560667\n",
      "75%     796.140613\n",
      "max    2366.456581\n",
      "                 0\n",
      "count   485.000000\n",
      "mean   1319.628002\n",
      "std     697.310573\n",
      "min       0.000000\n",
      "25%     800.629210\n",
      "50%    1191.894250\n",
      "75%    1697.159243\n",
      "max    3500.072065\n",
      "+2h-prediction test RMSE: 664.487\n",
      "pvlib forecast test RMSE: 1492.981\n",
      "+2h-prediction nice day RMSE: 290.691\n",
      "pvlib forecast nice day RMSE: 444.823\n",
      "daily mean +3h-prediction RMSE: 727.1679221059957\n",
      "daily mean pvlib forecast RMSE: 1319.6280018979849\n",
      "                 0\n",
      "count   485.000000\n",
      "mean    727.167922\n",
      "std     490.541319\n",
      "min      23.495796\n",
      "25%     398.250047\n",
      "50%     671.909034\n",
      "75%     993.582295\n",
      "max    4975.181518\n",
      "                 0\n",
      "count   485.000000\n",
      "mean   1319.628002\n",
      "std     697.310573\n",
      "min       0.000000\n",
      "25%     800.629210\n",
      "50%    1191.894250\n",
      "75%    1697.159243\n",
      "max    3500.072065\n",
      "+3h-prediction test RMSE: 877.933\n",
      "pvlib forecast test RMSE: 1492.981\n",
      "+3h-prediction nice day RMSE: 436.116\n",
      "pvlib forecast nice day RMSE: 444.823\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for i in range(forecast_horizon):\n",
    "    a = np.empty(i)\n",
    "    a.fill(np.nan)\n",
    "    b = np.empty(forecast_horizon - i - 1)\n",
    "    b.fill(np.nan)\n",
    "    data['+'+str(i+1)+'h-prediction'] = np.append(np.append(a, prediction[:][0][i].values), b)\n",
    "data['measured'] = pd.DataFrame(np.array(testY).reshape([len(testY), len(target_features)])).iloc[:,0]\n",
    "data = data.set_index(pd.MultiIndex.from_tuples(idx[:-2])).unstack()#[:-4]\n",
    "data['pvlib'] = pvlib['2015-10-12 07:00:00':'2017-02-09 10:00:00'].reindex(data.index)\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp[method] = data[method]\n",
    "tmp['measured'] = data[('measured', 4.0)]\n",
    "for i in range(forecast_horizon):\n",
    "    tmp['+'+str(i+1)+'h-prediction'] = data['+'+str(i+1)+'h-prediction']\n",
    "data = tmp\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.dropna()\n",
    "\n",
    "m_col = data['measured']\n",
    "l_col = data[method].dropna()\n",
    "\n",
    "for horizon in range(1, forecast_horizon + 1):\n",
    "    name = '+' + str(horizon) + 'h-prediction'\n",
    "    p_col = data[name]\n",
    "\n",
    "    walkForwardDailyLoss(m_col, p_col, l_col, method, name)\n",
    "    scatter_predictions(m_col, p_col, name)\n",
    "\n",
    "    print('%s test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "    print('%s test RMSE: %.3f' % (method + ' forecast', math.sqrt(mean_squared_error(m_col, l_col))))\n",
    "    draw_boxplot(m_col, p_col, l_col, method, name)\n",
    "    draw_boxplot_monthly(m_col, p_col, l_col, method, name)\n",
    "\n",
    "    m1, m2 = '2016-07-17 00:00:00', '2016-07-17 23:00:00'\n",
    "    print('%s nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))\n",
    "    print('%s nice day RMSE: %.3f' % (method + ' forecast', math.sqrt(mean_squared_error(m_col[m1:m2], l_col[m1:m2]))))\n",
    "    draw_boxplot(m_col, p_col, l_col, method, name, m1, m2)\n",
    "\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, end='2015-10-19 07:00:00')\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, start='2017-02-02 10:00:00')\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, start=m1, end=m2)\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name)\n",
    "    plot_timeseries(m_col, p_col, None, method, name)\n",
    "\n",
    "    draw_histogram(p_col, m_col, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_history(history)\n",
    "draw_history(val_history, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pvlib      measured  +1h-prediction  +2h-prediction  \\\n",
      "count  11662.000000  11662.000000    11662.000000    11662.000000   \n",
      "mean    1549.683537   1065.446782     1029.952963     1034.587587   \n",
      "std     2226.383739   1925.252500     1887.771854     1853.179978   \n",
      "min        0.000000      0.000000    -1551.626465    -2449.488281   \n",
      "25%        0.000000      0.000000        0.000003        0.000092   \n",
      "50%        0.000000      0.000000        4.464455        7.287639   \n",
      "75%     2846.756628   1137.207027     1166.288116     1253.534210   \n",
      "max     9849.600000   8494.140630     9970.578125     9347.826172   \n",
      "\n",
      "       +3h-prediction  \n",
      "count    11662.000000  \n",
      "mean      1011.034874  \n",
      "std       1831.752629  \n",
      "min     -20399.146484  \n",
      "25%          0.000096  \n",
      "50%         13.068636  \n",
      "75%       1243.079102  \n",
      "max      10553.772461  \n",
      "                   pvlib  measured  +1h-prediction  +2h-prediction  \\\n",
      "pvlib           1.000000  0.777900        0.784259        0.790481   \n",
      "measured        0.777900  1.000000        0.968026        0.938978   \n",
      "+1h-prediction  0.784259  0.968026        1.000000        0.967232   \n",
      "+2h-prediction  0.790481  0.938978        0.967232        1.000000   \n",
      "+3h-prediction  0.777437  0.892370        0.924320        0.960327   \n",
      "\n",
      "                +3h-prediction  \n",
      "pvlib                 0.777437  \n",
      "measured              0.892370  \n",
      "+1h-prediction        0.924320  \n",
      "+2h-prediction        0.960327  \n",
      "+3h-prediction        1.000000  \n",
      "                   pvlib  measured  +1h-prediction  +2h-prediction  \\\n",
      "pvlib           1.000000  0.904257        0.820351        0.812004   \n",
      "measured        0.904257  1.000000        0.891186        0.879011   \n",
      "+1h-prediction  0.820351  0.891186        1.000000        0.857710   \n",
      "+2h-prediction  0.812004  0.879011        0.857710        1.000000   \n",
      "+3h-prediction  0.798023  0.853618        0.835337        0.925287   \n",
      "\n",
      "                +3h-prediction  \n",
      "pvlib                 0.798023  \n",
      "measured              0.853618  \n",
      "+1h-prediction        0.835337  \n",
      "+2h-prediction        0.925287  \n",
      "+3h-prediction        1.000000  \n",
      "                   pvlib  measured  +1h-prediction  +2h-prediction  \\\n",
      "pvlib           1.000000  0.778644        0.655083        0.646878   \n",
      "measured        0.778644  1.000000        0.770612        0.742326   \n",
      "+1h-prediction  0.655083  0.770612        1.000000        0.722920   \n",
      "+2h-prediction  0.646878  0.742326        0.722920        1.000000   \n",
      "+3h-prediction  0.636157  0.705638        0.681965        0.827607   \n",
      "\n",
      "                +3h-prediction  \n",
      "pvlib                 0.636157  \n",
      "measured              0.705638  \n",
      "+1h-prediction        0.681965  \n",
      "+2h-prediction        0.827607  \n",
      "+3h-prediction        1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data.describe())\n",
    "print(data.corr(method='pearson'))\n",
    "print(data.corr(method='spearman'))\n",
    "print(data.corr(method='kendall'))\n",
    "data.to_csv(dir + 'predictions.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
