{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import TensorBoard, History, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from evaluation import *\n",
    "from machineLearningModel import *\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)\n",
    "\n",
    "## net params\n",
    "num_layers = 4#2#1#4#3#6\n",
    "num_neurons = 300#50#100#500\n",
    "batch_size = 100#500#1000\n",
    "dropout_rate = 0\n",
    "const_features = ['latitude', 'longitude', 'altitude', 'modules_per_string', 'strings_per_inverter', 'tilt',\n",
    "                  'azimuth', 'albedo', 'Technology', 'BIPV', 'A_c', 'N_s', 'pdc0', 'gamma_pdc']#, 'SystemID']#15\n",
    "dyn_features = ['Wind Direction_x', 'Wind Direction_y', 'Total Cloud Cover', 'Low Cloud Cover', 'Medium Cloud Cover',\n",
    "                'High Cloud Cover', 'Wind Speed', 'Wind Gust', 'Total Precipitation',\n",
    "                'Snow Fraction', 'Mean Sea Level Pressure', 'DIF - backwards', 'DNI - backwards', 'Shortwave Radiation',\n",
    "                'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']#20\n",
    "#const_features = ['SystemID']\n",
    "#dyn_features = ['DIF - backwards', 'DNI - backwards', 'Shortwave Radiation', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']#, 'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']\n",
    "target_features = ['power']\n",
    "drop_features = ['power_pvlib']\n",
    "act_fct = 'relu'\n",
    "out_act = 'relu' # linear, relu\n",
    "loss_fct = 'mae' # mse, mae\n",
    "optim = 'adam'\n",
    "metrics = []\n",
    "history = History()\n",
    "val_history = History()\n",
    "\n",
    "## data params\n",
    "filename = './data/full_data_5_systems.csv'\n",
    "correlations = []#'pearson', 'spearman', 'kendall']\n",
    "timesteps = 5#24\n",
    "method = 'convlstm' # randfor, mlp, lstm, dilated, cnnlstm, convlstm\n",
    "flat = ['randfor', 'mlp']\n",
    "if method in flat:\n",
    "    shape = (len(const_features + dyn_features) + timesteps * (len(dyn_features + target_features)) + 1,)\n",
    "else:\n",
    "    #shape = (timesteps + 1, len(const_features + dyn_features))\n",
    "    shape = (timesteps + 1, len(const_features + dyn_features + target_features))\n",
    "\n",
    "## training params\n",
    "pretraining = True\n",
    "wfvtraining = True\n",
    "tensorboard = False\n",
    "callbacks = [history, ModelCheckpoint('./saved_models/best_model.h5', save_best_only=True)]#, verbose=1)] #EarlyStopping(patience=5), \n",
    "shuffle = False#True\n",
    "epochs = 200#50#20\n",
    "val_split = 1.0/10.0\n",
    "forecast_horizon = 1#6#3#24\n",
    "sliding_window = 672#336#8760#672#336#24#8760\n",
    "dir = './test_results/'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suffix = ''\n",
    "if method not in flat:\n",
    "    suffix = '_2D'\n",
    "pfname = dir + 'preprocessed_data_t-'+str(timesteps)+'_f'+str(shape[0])+suffix+'.csv'\n",
    "prep = Path(pfname)\n",
    "if prep.exists():\n",
    "    print('Loading preprocessed dataset ...')\n",
    "    pvlib = np.array_split(pd.read_csv(filename, skipinitialspace=True).set_index(['time', 'SystemID']), 5)[-1].power_pvlib\n",
    "    dataset = pd.read_csv(pfname, skipinitialspace=True)#.set_index(['time', 'SystemID'])\n",
    "else:\n",
    "    print('Data preprocessing ...')\n",
    "    df = pd.read_csv(filename, skipinitialspace=True).set_index(['time', 'SystemID'])#.set_index('time')\n",
    "    df = np.array_split(df, 5)[-1] ##################################\n",
    "    pvlib = df.power_pvlib\n",
    "    dataset = df[const_features + dyn_features + target_features].copy()[:'2017-02-09 10:00:00']\n",
    "\n",
    "    if method in flat:\n",
    "        #separate system\n",
    "        for i in range(1, timesteps + 1):\n",
    "            for feature in dyn_features + target_features:\n",
    "                sys.stdout.write(\"Shifting %i/%i %s                \\r\" % (i, timesteps, feature))\n",
    "                sys.stdout.flush()\n",
    "                dataset[feature + ' t-' + str(i)] = dataset.shift(i)[feature]\n",
    "        print('Shifting done.                ')\n",
    "\n",
    "        dataset['forecast_horizon'] = 0\n",
    "        p = dataset[target_features]\n",
    "        dataset = dataset.drop(target_features, axis=1)\n",
    "        for f in target_features:\n",
    "            dataset[f] = p[f]\n",
    "        dataset = dataset.dropna()#.reset_index().set_index(['time', 'SystemID'])\n",
    "        \n",
    "        sys.stdout.write(\"Writing to file ...\\r\")\n",
    "        sys.stdout.flush()\n",
    "        dataset.to_csv(pfname, encoding='utf-8')\n",
    "        print('Writing done.                ')\n",
    "\n",
    "        if correlations:\n",
    "            sys.stdout.write('Computing correlations ...\\r')\n",
    "            sys.stdout.flush()\n",
    "            for corr in correlations:\n",
    "                sys.stdout.write(\"Computing %s correlation matrix                \\r\" % (corr))\n",
    "                sys.stdout.flush()\n",
    "                dataset.corr(method=corr).to_csv(dir + corr + '_correlations.csv', encoding='utf-8')\n",
    "            print('Correlations done.                   ')\n",
    "    else:\n",
    "        x = []\n",
    "        for i in range(timesteps+1, len(dataset)+1):# t-24h dazu?\n",
    "            d = dataset.iloc[i-timesteps-1:i].copy()\n",
    "            d.iloc[-1, -len(target_features):] = -1 # oder auf pvlib wert setzen?\n",
    "            x.append(d.values)\n",
    "            #x.append(dataset.iloc[i-timesteps-1:i, :-len(target_features)].values)\n",
    "        x = np.array(x)\n",
    "        y = dataset[target_features].iloc[timesteps:]\n",
    "        split = dataset[:('2015-10-12 07:00:00', 4.0)].iloc[timesteps+1:].shape[0]\n",
    "        trainX, testX = x[:split], x[split:]\n",
    "        trainY, testY = y.iloc[:split].values, y.iloc[split:].values\n",
    "        idx = y.iloc[split:].index\n",
    "        # stimmt bis hier\n",
    "\n",
    "if method in flat:\n",
    "    train, test = dataset[:('2015-10-12 06:00:00', 4.0)], dataset[('2015-10-12 07:00:00', 4.0):]\n",
    "    trainX, trainY = train.iloc[:,:-len(target_features)], train.iloc[:,-len(target_features):]\n",
    "    testX, testY = test.iloc[:,:-len(target_features)], test.iloc[:,-len(target_features):]\n",
    "    idx = testX.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method is 'randfor':\n",
    "    model = RandomForest(10, 0.33, 'mae')#216 0.63 mse\n",
    "else:\n",
    "    if tensorboard:\n",
    "        print('tensorboard activated')\n",
    "        callbacks.append(TensorBoard(log_dir='./tensorboard', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False))\n",
    "\n",
    "    if method is 'mlp':\n",
    "        model = MultiLayerPerceptron(shape, len(target_features), num_layers, num_neurons, loss_fct, optim,\n",
    "                                     act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                                     epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'lstm':\n",
    "        model = LongShortTermMemory(shape, len(target_features), num_layers, num_neurons, loss_fct, optim,\n",
    "                                    act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                                    epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'dilated':\n",
    "        model = DilatedConvolution(shape, len(target_features), num_layers, loss_fct, optim,\n",
    "                                   act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                                   epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'cnnlstm':\n",
    "        model = CNNLSTM(shape, len(target_features), num_layers, num_neurons, loss_fct, optim,\n",
    "                        act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                        epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'convlstm':\n",
    "        model = ConvLSTM(shape, len(target_features), num_layers, num_neurons, loss_fct, optim,\n",
    "                         act_fct, out_act, metrics, dropout_rate, dir + 'model.png', batch_size,\n",
    "                         epochs, val_split, callbacks, 1, True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if pretraining:\n",
    "    X = testX#trainX\n",
    "    y = testY#trainY\n",
    "\n",
    "    if shuffle:\n",
    "        df = pd.DataFrame(np.concatenate((trainX, trainY), axis=1))\n",
    "        df = df.sample(frac=1).values\n",
    "        y = pd.DataFrame(df[:, -len(target_features):])\n",
    "        X = pd.DataFrame(df[:, :-len(target_features)])\n",
    "    if method is 'cnnlstm':\n",
    "        X = X.reshape((X.shape[0], 1, shape[0], shape[1]))\n",
    "    if method is 'convlstm':\n",
    "        X = X.reshape((X.shape[0], 1, 1, shape[0], shape[1]))\n",
    "        \n",
    "    print('Start pretraining ...')    \n",
    "    model.learn(X, y)\n",
    "\n",
    "    if method != 'randfor':\n",
    "        name = './saved_models/pretrained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)+'_sys'+str(4)\n",
    "        # serialize model to JSON\n",
    "        model_json = model.model.to_json()\n",
    "        with open(name + \".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.model.save_weights(name + \".h5\")\n",
    "        print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretrained model for different testing\n",
    "model_backup = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best\n",
    "model.model.load_weights('./saved_models/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk-Forward Validation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.epochs = 5\n",
    "model.verbose = 0\n",
    "model.callbacks = [val_history] \n",
    "model.validation_split = 0.0\n",
    "\n",
    "predictions = []\n",
    "pred_err = []\n",
    "length = len(testX)# - forecast_horizon - 1\n",
    "trainset = []\n",
    "trainy = []\n",
    "perr = -1\n",
    "for i in range(length):\n",
    "    if i == sliding_window + 1:\n",
    "        model.validation_split = 1.0/10.0\n",
    "        model.callbacks.append(EarlyStopping(patience=5))\n",
    "        #model.epochs = 5\n",
    "    if i > sliding_window + 1 and wfvtraining:\n",
    "        loss = h.history['loss'][-1]\n",
    "        val_loss = min(h.history['val_loss'])#[-1]#min()\n",
    "    else:\n",
    "        loss = -1\n",
    "        val_loss = -1\n",
    "\n",
    "    sys.stdout.write(\"Walk-Forward Validation %i/%i: %d %d, pred: %d    \\r\" % (i+1, length, loss, val_loss, perr))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if method in flat:\n",
    "        # initialize values for lagged power columns\n",
    "        p = []\n",
    "        for l in range(1, timesteps + 1):\n",
    "            p.append(testX.iloc[i:i+1,:]['power t-'+str(l)].values[0])\n",
    "\n",
    "        ps = []\n",
    "        ts = []\n",
    "        ty = []\n",
    "        for f in range(forecast_horizon):\n",
    "            # build input vector for future timestep\n",
    "            t = testX.iloc[i+f:i+1+f,:].copy()\n",
    "            for l in range(timesteps-1, 1, -1):\n",
    "                t['power t-' + str(l+1)] = p[l]\n",
    "                p[l] = p[l-1]\n",
    "            t['power t-1'] = p[0]\n",
    "            t['forecast_horizon'] = f\n",
    "            ts.append(t)\n",
    "            ty.append(testY.iloc[i+f:i+1+f,:])\n",
    "\n",
    "            # make prediction for input new vector\n",
    "            p[0] = model.forecast(t.values).item(0)\n",
    "            ps.append(p[0])\n",
    "\n",
    "        pred_err.append((testY.iloc[i:i+forecast_horizon,:].values - ps).flatten())\n",
    "        perr = pred_err[-1][0]\n",
    "        \n",
    "        trainset = trainset + ts\n",
    "        trainy = trainy + ty\n",
    "        w = i - sliding_window\n",
    "        if w >= 0:\n",
    "            trainset = trainset[forecast_horizon:]\n",
    "            trainy = trainy[forecast_horizon:]\n",
    "\n",
    "        if wfvtraining:\n",
    "            dfX = pd.concat(trainset)\n",
    "            dfX = dfX[dfX.index.get_level_values(0) <= testX.iloc[i].name[0]]\n",
    "            dfY = pd.concat(trainy)\n",
    "            dfY = dfY[dfY.index.get_level_values(0) <= testY.iloc[i].name[0]]\n",
    "\n",
    "            # train with newly available data\n",
    "            loss = model.learn(dfX.values, dfY.values)\n",
    "\n",
    "        predictions.append(pd.DataFrame(ps))\n",
    "    else:\n",
    "        # collect predictions\n",
    "        X = testX[i:i+forecast_horizon,:]\n",
    "        if method is 'cnnlstm':\n",
    "            X = X.reshape((X.shape[0], 1, shape[0], shape[1]))\n",
    "        if method is 'convlstm':\n",
    "            X = X.reshape((X.shape[0], 1, 1, shape[0], shape[1]))\n",
    "        p = pd.DataFrame(model.forecast(X))\n",
    "        predictions.append(p)\n",
    "        pred_err.append(testY[i:i+forecast_horizon,:] - p)\n",
    "        perr = pred_err[-1][0]\n",
    "       \n",
    "        if wfvtraining:\n",
    "            w = i - sliding_window\n",
    "            if w < 0:\n",
    "                w = 0\n",
    "\n",
    "            #if w >= 0:\n",
    "            X = testX[w:i+1]\n",
    "            if method is 'cnnlstm':\n",
    "                X = X.reshape((X.shape[0], 1, shape[0], shape[1]))\n",
    "            if method is 'convlstm':\n",
    "                X = X.reshape((X.shape[0], 1, 1, shape[0], shape[1]))\n",
    "\n",
    "            # train with newly available data\n",
    "            h = model.learn(X, testY[w:i+1])#forecast_horizon? only if y is not zero maybe?\n",
    "    \n",
    "prediction = pd.concat(predictions)\n",
    "\n",
    "if method != 'randfor':\n",
    "    name = './saved_models/trained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)+'_sys'+str(4)\n",
    "    # serialize model to JSON\n",
    "    model_json = model.model.to_json()\n",
    "    with open(name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.model.save_weights(name + \".h5\")\n",
    "    print(\"\\nSaved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.abs(pred_err))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'pvlib'\n",
    "data = pd.DataFrame()\n",
    "for i in range(forecast_horizon):\n",
    "    data['+'+str(i+1)+'h-prediction'] = np.pad(prediction[:][0][i].values, (i, 0), mode='constant', constant_values=(np.nan,))\n",
    "    #data['+'+str(i+1)+'h-prediction'] = np.pad(prediction[:][0][i].values, (i, forecast_horizon - i - 1), mode='constant', constant_values=(np.nan,))\n",
    "data['measured'] = pd.DataFrame(np.array(testY).reshape([len(testY), len(target_features)])).iloc[:,0]\n",
    "data = data.set_index(pd.MultiIndex.from_tuples(idx)).unstack()#[:-4] [:-2] [:1-forecast_horizon]\n",
    "data['pvlib'] = pvlib[('2015-10-12 07:00:00', 4.0):('2017-02-09 10:00:00', 4.0)].unstack()[4].reindex(data.index)\n",
    "print(data)\n",
    "#print(asdf)\n",
    "# stimmt bis hier\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp[method] = data[method]\n",
    "tmp['measured'] = data[('measured', 4.0)]\n",
    "for i in range(forecast_horizon):\n",
    "    tmp['+'+str(i+1)+'h-prediction'] = data['+'+str(i+1)+'h-prediction']\n",
    "data = tmp\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.dropna()\n",
    "\n",
    "m_col = data['measured']\n",
    "l_col = data[method].dropna()\n",
    "\n",
    "for horizon in range(1, forecast_horizon + 1):\n",
    "    name = '+' + str(horizon) + 'h-prediction'\n",
    "    p_col = data[name]\n",
    "\n",
    "    walkForwardDailyLoss(m_col, p_col, l_col, method, name)\n",
    "    scatter_predictions(m_col, p_col, name)\n",
    "\n",
    "    print('%s test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "    print('%s test RMSE: %.3f' % (method + ' forecast', math.sqrt(mean_squared_error(m_col, l_col))))\n",
    "    draw_boxplot(m_col, p_col, l_col, method, name)\n",
    "    draw_boxplot_monthly(m_col, p_col, l_col, method, name)\n",
    "\n",
    "    m1, m2 = '2016-07-17 00:00:00', '2016-07-17 23:00:00'\n",
    "    print('%s nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))\n",
    "    print('%s nice day RMSE: %.3f' % (method + ' forecast', math.sqrt(mean_squared_error(m_col[m1:m2], l_col[m1:m2]))))\n",
    "    draw_boxplot(m_col, p_col, l_col, method, name, m1, m2)\n",
    "\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, end='2015-10-19 07:00:00')\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, start='2017-02-02 10:00:00')\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name, start=m1, end=m2)\n",
    "    plot_timeseries(m_col, p_col, l_col, method, name)\n",
    "    plot_timeseries(m_col, p_col, None, method, name)\n",
    "\n",
    "    draw_histogram(p_col, m_col, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_history(history)\n",
    "draw_history(val_history, True)\n",
    "print(val_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "print(data.corr(method='pearson'))\n",
    "print(data.corr(method='spearman'))\n",
    "print(data.corr(method='kendall'))\n",
    "data.to_csv(dir + 'predictions.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
