{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "numpy 1.14.3\n",
      "pandas 0.23.0\n",
      "matplotlib 2.2.2\n",
      "sklearn 0.19.1\n",
      "keras 2.2.4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard, History, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from evaluation import *\n",
    "from machineLearningModel import *\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "print(sys.version)\n",
    "print('numpy ' + np.__version__)\n",
    "print('pandas ' + pd.__version__)\n",
    "print('matplotlib ' + matplotlib.__version__)\n",
    "print('sklearn ' + sklearn.__version__)\n",
    "print('keras ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(13)\n",
    "\n",
    "## net params\n",
    "num_layers = 3#2#2#1#4#3#6\n",
    "num_neurons = 100#100#10#50#300#50#100#500\n",
    "batch_size = 1024#128#500#1000\n",
    "dropout_rate = 0\n",
    "const_features = ['latitude', 'longitude', 'altitude', 'modules_per_string', 'strings_per_inverter', 'tilt',\n",
    "                  'azimuth', 'albedo', 'Technology', 'BIPV', 'A_c', 'N_s', 'pdc0', 'gamma_pdc']#, 'SystemID']#15\n",
    "dyn_features = ['Wind Direction_x', 'Wind Direction_y', 'Total Cloud Cover', 'Low Cloud Cover', 'Medium Cloud Cover',\n",
    "                'High Cloud Cover', 'Wind Speed', 'Wind Gust', 'Total Precipitation',\n",
    "                'Snow Fraction', 'Mean Sea Level Pressure', 'DIF - backwards', 'DNI - backwards', 'Shortwave Radiation',\n",
    "                'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']#20\n",
    "#const_features = ['latitude', 'longitude']#'SystemID']\n",
    "#dyn_features = ['DIF - backwards', 'DNI - backwards', 'Shortwave Radiation', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']#, 'Temperature', 'Relative Humidity', 'Hour_x', 'Hour_y', 'Month_x', 'Month_y']\n",
    "target_features = ['power']\n",
    "drop_features = ['power_pvlib']\n",
    "act_fct = 'relu'\n",
    "out_act = 'linear' # linear, relu\n",
    "loss_fct = 'mae' # mse, mae\n",
    "optim = 'adam'#Adam(lr=0.00001)#'adam'\n",
    "metrics = []\n",
    "history = History()\n",
    "val_history = History()\n",
    "\n",
    "## data params\n",
    "filename = './data/full_data_5_systems.csv'\n",
    "correlations = []#'pearson', 'spearman', 'kendall']\n",
    "timesteps = 5#24#5#1#12#5#1#3#2#1#5#168#2190#336#72#24#5#24\n",
    "method = 'dilated' # randfor, mlp, lstm, dilated\n",
    "flat = ['randfor', 'mlp']\n",
    "recursive = True#False\n",
    "num_sys = 5\n",
    "\n",
    "## training params\n",
    "pretraining = True\n",
    "wfvtraining = True\n",
    "tensorboard = False\n",
    "callbacks = [history, EarlyStopping(patience=10, restore_best_weights=True), ModelCheckpoint('./saved_models/best_model.h5', save_best_only=True)]#, verbose=1)] #EarlyStopping(patience=5), \n",
    "shuffle = False\n",
    "epochs = 200#50#20\n",
    "forecast_horizon = 24#3#1#6#24#3#6#3#24\n",
    "sliding_window = 169#8760#672#3#8#24#72#672#336#8760#672#336#24#8760\n",
    "dir_ = './test_results/'\n",
    "if not os.path.exists(dir_):\n",
    "    os.makedirs(dir_)\n",
    "\n",
    "# input shape\n",
    "if method in flat:\n",
    "    if recursive:\n",
    "        shape = (len(const_features + dyn_features) + timesteps * (len(dyn_features + target_features)) + 1,)\n",
    "    else:\n",
    "        shape = (len(const_features) + timesteps * (len(dyn_features + target_features)) + forecast_horizon * len(dyn_features),)\n",
    "else:\n",
    "    if recursive:\n",
    "        shape = (timesteps + 1, len(const_features + dyn_features + target_features) + 1)\n",
    "    else:\n",
    "        #shape = (timesteps, len(const_features + target_features) + len(dyn_features) * (forecast_horizon + timesteps))\n",
    "        shape = (timesteps + forecast_horizon, len(const_features + dyn_features + target_features))\n",
    "\n",
    "# output shape\n",
    "if recursive:\n",
    "    out_dim = len(target_features)\n",
    "else:\n",
    "    out_dim = forecast_horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset ...\n",
      "Preprocessing done.\n"
     ]
    }
   ],
   "source": [
    "suffix1 = ''\n",
    "suffix2 = ''\n",
    "if method in flat:\n",
    "    suffix1 = '_flat'\n",
    "if not recursive:\n",
    "    suffix2 = '_fixed'\n",
    "fname = dir_ + 'data_step' + str(timesteps) + suffix1 + suffix2\n",
    "\n",
    "if Path(fname + '.npz').exists():\n",
    "    print('Loading preprocessed dataset ...')\n",
    "    with np.load(fname + '.npz') as datafile:\n",
    "        trainX = datafile['trainX']\n",
    "        trainY = datafile['trainY']\n",
    "        testX = datafile['testX']\n",
    "        testY = datafile['testY']\n",
    "        pvlib = datafile['pvlib']\n",
    "        idx = datafile['idx']\n",
    "else:\n",
    "    print('Data preprocessing ...')\n",
    "    dataframe = pd.read_csv(filename, skipinitialspace=True).set_index(['time', 'SystemID'])\n",
    "    dataframe = np.array_split(dataframe, num_sys)\n",
    "    pvlibs = []\n",
    "    trainXs = []\n",
    "    trainYs = []\n",
    "    testXs = []\n",
    "    testYs = []\n",
    "    idxs = []\n",
    "    for s in range(num_sys-1, -1, -1):\n",
    "        df = dataframe[s]\n",
    "        pvlibs.append(df.power_pvlib)\n",
    "        dataset = df[const_features + dyn_features + target_features]\n",
    "\n",
    "        if method in flat:\n",
    "            if recursive:\n",
    "                for i in range(1, timesteps + 1):\n",
    "                    for feature in dyn_features + target_features:\n",
    "                        sys.stdout.write(\"Shifting %1i/%i %24s\\r\" % (i, timesteps, feature))\n",
    "                        sys.stdout.flush()\n",
    "                        dataset[feature + ' t-' + str(i)] = dataset.shift(i)[feature]\n",
    "                print('Shifting done.                ')\n",
    "\n",
    "                dataset['forecast_horizon'] = 0\n",
    "                p = dataset[target_features]\n",
    "                dataset = dataset.drop(target_features, axis=1)\n",
    "                for f in target_features:\n",
    "                    dataset[f] = p[f]\n",
    "                dataset = dataset.dropna()\n",
    "\n",
    "                train, test = dataset[:('2015-10-10 23:00:00', s)], dataset[('2015-10-11 00:00:00', s):]\n",
    "                trainX, trainY = train.iloc[:,:-len(target_features)], train.iloc[:,-len(target_features):]\n",
    "                testX, testY = test.iloc[:,:-len(target_features)], test.iloc[:,-len(target_features):]\n",
    "                idx = testX.index.values\n",
    "            else:\n",
    "                for i in range(forecast_horizon, 1, -1):\n",
    "                    for feature in dyn_features:\n",
    "                        sys.stdout.write(\"Shifting %1i/%i %24s \\r\" % (i, forecast_horizon, feature))\n",
    "                        sys.stdout.flush()\n",
    "                        dataset[feature + ' t+' + str(i)] = dataset.shift(i)[feature]\n",
    "\n",
    "                for i in range(1, timesteps + 1):\n",
    "                    for feature in dyn_features + target_features:\n",
    "                        sys.stdout.write(\"Shifting %1i/%i %25s \\r\" % (i, timesteps, feature))\n",
    "                        sys.stdout.flush()\n",
    "                        dataset[feature + ' t-' + str(i)] = dataset.shift(i)[feature]\n",
    "                print('Shifting done.                                ')\n",
    "\n",
    "                train, test = dataset[:('2015-10-10 23:00:00', s)], dataset[('2015-10-11 00:00:00', s):]\n",
    "                trainX = train.drop(target_features, axis=1)\n",
    "                trainY = [train[target_features].iloc[i:i+forecast_horizon].values.flatten() for i in range(len(train))]\n",
    "\n",
    "                testX = test.drop(target_features, axis=1)\n",
    "                testY = [test[target_features].iloc[i:i+forecast_horizon].values.flatten() for i in range(len(test))]\n",
    "\n",
    "                for f in range(1, forecast_horizon):\n",
    "                    trainY[-f] = np.pad(trainY[-f], (0, forecast_horizon-f), mode='constant', constant_values=(np.nan,))\n",
    "                    testY[-f] = np.pad(testY[-f], (0, forecast_horizon-f), mode='constant', constant_values=(np.nan,))\n",
    "                trainY = np.array(trainY[:-forecast_horizon])\n",
    "                trainX = trainX[:-forecast_horizon]\n",
    "                testY = np.array(testY[:-forecast_horizon])\n",
    "                testX = testX[:-forecast_horizon]\n",
    "                idx = testX.index.values\n",
    "        else:\n",
    "            if recursive:\n",
    "                dataset['forecast_horizon'] = 0\n",
    "                p = dataset[target_features]\n",
    "                dataset = dataset.drop(target_features, axis=1)\n",
    "                for f in target_features:\n",
    "                    dataset[f] = p[f]\n",
    "                dataset = dataset.dropna()\n",
    "\n",
    "                x = []\n",
    "                for i in range(timesteps+1, len(dataset)+1):\n",
    "                    sys.stdout.write(\"System %i/%i: %5i/%i                \\r\" % (s+1, num_sys, i, len(dataset)))\n",
    "                    sys.stdout.flush()\n",
    "                    d = dataset.iloc[i-timesteps-1:i].copy()\n",
    "                    d.iloc[-1, -len(target_features):] = -1\n",
    "                    x.append(d.values)\n",
    "                x = np.array(x)\n",
    "                y = dataset[target_features].iloc[timesteps:]\n",
    "                split = dataset[:('2015-10-11 00:00:00', s)].iloc[timesteps+1:].shape[0]#'2015-10-12 07:00:00'\n",
    "                trainX, testX = x[:split], x[split:]\n",
    "                trainY, testY = y.iloc[:split].values, y.iloc[split:].values\n",
    "                idx = y.iloc[split:].index\n",
    "            else:\n",
    "                x = []\n",
    "                y = []\n",
    "                for i in range(timesteps, len(dataset)-forecast_horizon):\n",
    "                    sys.stdout.write(\"System %i/%i: %5i/%i                \\r\" % (s+1, num_sys, i, len(dataset)))\n",
    "                    sys.stdout.flush()\n",
    "                    d = dataset.iloc[i-timesteps:i+forecast_horizon].copy()\n",
    "                    d.iloc[-forecast_horizon:, -len(target_features):] = -1\n",
    "                    x.append(d.values)\n",
    "                    y.append(dataset.iloc[i:i+forecast_horizon][target_features].values)\n",
    "                x = np.array(x)\n",
    "                y = np.array(y)\n",
    "                y = y.reshape(y.shape[0], y.shape[1])\n",
    "\n",
    "                split = dataset[:('2015-10-10 23:00:00', s)].iloc[timesteps:].shape[0]\n",
    "                trainX, testX = x[:split-forecast_horizon], x[split:]\n",
    "                trainY, testY = y[:split-forecast_horizon], y[split:]\n",
    "                idx = dataset[('2015-10-10 23:00:00', s):].index[1:-1]\n",
    "        trainXs.append(trainX)\n",
    "        trainYs.append(trainY)\n",
    "        testXs.append(testX)\n",
    "        testYs.append(testY)\n",
    "        idxs.append(idx)\n",
    "\n",
    "    a = np.stack(trainYs, axis=1)\n",
    "    trainY = a.reshape(a.shape[0]*a.shape[1], a.shape[2])\n",
    "\n",
    "    a = np.stack(trainXs, axis=1)\n",
    "    if method in flat:\n",
    "        trainX = a.reshape(a.shape[0]*a.shape[1], a.shape[2])\n",
    "    else:\n",
    "        trainX = a.reshape(a.shape[0]*a.shape[1], a.shape[2], a.shape[3])\n",
    "\n",
    "    a = np.stack(testYs, axis=1)\n",
    "    testY = a.reshape(a.shape[0]*a.shape[1], a.shape[2])\n",
    "\n",
    "    a = np.stack(testXs, axis=1)\n",
    "    if method in flat:\n",
    "        testX = a.reshape(a.shape[0]*a.shape[1], a.shape[2])\n",
    "    else:\n",
    "        testX = a.reshape(a.shape[0]*a.shape[1], a.shape[2], a.shape[3])\n",
    "\n",
    "    a = np.stack(pvlibs, axis=1)\n",
    "    if recursive:\n",
    "        pvlib = a.reshape(a.shape[0]*a.shape[1], 1)[-len(testY):]\n",
    "    else:\n",
    "        pvlib = a.reshape(a.shape[0]*a.shape[1], 1)[-len(testY)-forecast_horizon*num_sys:]\n",
    "\n",
    "    a = np.stack(idxs, axis=1)\n",
    "    idx = a.reshape(a.shape[0]*a.shape[1])\n",
    "\n",
    "    np.savez(fname, trainX=trainX, trainY=trainY, testX=testX, testY=testY, pvlib=pvlib, idx=idx)\n",
    "    print('Saved to ' + fname + '.npz       ')\n",
    "print('Preprocessing done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method is 'randfor':\n",
    "    model = RandomForest(75, 'auto', 'mse', verbose=0)#216 0.63 mse   10, 0.33, 'mae',  200, 0.13, 'mse': 597\n",
    "else:\n",
    "    if tensorboard:\n",
    "        print('tensorboard activated')\n",
    "        callbacks.append(TensorBoard(log_dir='./tensorboard', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False))\n",
    "\n",
    "    if method is 'mlp':\n",
    "        model = MultiLayerPerceptron(shape, out_dim, num_layers, num_neurons, loss_fct, optim,\n",
    "                                     act_fct, out_act, metrics, dropout_rate, dir_ + 'model.png', batch_size,\n",
    "                                     epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'lstm':\n",
    "        model = LongShortTermMemory(shape, out_dim, num_layers, num_neurons, loss_fct, optim,\n",
    "                                    act_fct, out_act, metrics, dropout_rate, dir_ + 'model.png', batch_size,\n",
    "                                    epochs, val_split, callbacks, 1, True)\n",
    "    elif method is 'dilated':\n",
    "        model = DilatedConvolution(shape, out_dim, 3, num_neurons, 2, 36, 'causal', loss_fct, optim, #best: layers 3, ks 2, filters 32, 'causal'\n",
    "                                   act_fct, out_act, metrics, dropout_rate, dir_ + 'model.png', batch_size,\n",
    "                                   epochs, val_split, callbacks, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if pretraining:\n",
    "    X = trainX\n",
    "    y = trainY\n",
    "    \n",
    "    if shuffle:\n",
    "        rand_idx = np.array(np.arange(X.shape[0]))\n",
    "        np.random.shuffle(rand_idx)\n",
    "        X = X[rand_idx]\n",
    "        y = y[rand_idx]\n",
    "\n",
    "    print('Start pretraining ...')    \n",
    "    model.learn(X, y, val_idx=int(len(y) / 10.0))\n",
    "    print('Done.')\n",
    "\n",
    "    if method is not 'randfor':\n",
    "        name = './saved_models/pretrained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)#+'_sys'+str(system)\n",
    "        # serialize model to JSON\n",
    "        model_json = model.model.to_json()\n",
    "        with open(name + \".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.model.save_weights(name + \".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "    else:\n",
    "        pretrained = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method is not 'randfor':\n",
    "    # load best\n",
    "    model.model.load_weights('./saved_models/best_model.h5')\n",
    "else:\n",
    "    model = pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk-Forward Validation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dilated: 3 2 32 ep1 120w t-5#\n",
    "# mlp: 5 50 nich übel t-5#\n",
    "# lstm: \n",
    "# randfor: \n",
    "# fixed vs recursive ?#\n",
    "# batchsize = window?\n",
    "# vllt nicht auf -1 sondern -1000 setzen?#\n",
    "# fixer horizont vllt auch mit -1en?#\n",
    "# immer jedes 5. mal lernen (wenn alle systeme mit neuen daten versorgt)\n",
    "# filter auf predictions ganz am ende: negative werte bis +5 zu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method is 'randfor' and not pretraining:\n",
    "    model.learn(trainX[-1:], trainY[-1:], val_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.epochs = 1#1#50#200#50#1#50#5\n",
    "model.batch_size = 1024#1024#128\n",
    "model.verbose = 0\n",
    "model.callbacks = [val_history] \n",
    "model.shuffle = True#False#True\n",
    "wfvtraining = True\n",
    "sliding_window = 120#60#1680#120#840#168#1024#8760#168#24#169\n",
    "threshold = -2000#1#0#10 #200 100 150 500 1000 2000\n",
    "val_div = 2.0#2.0#10\n",
    "\n",
    "predictions = []\n",
    "pred_err = []\n",
    "length = len(testX) - forecast_horizon * num_sys#810#10100#10000#len(testX)#3000#2600\n",
    "trainset = []\n",
    "trainy = []\n",
    "perr = -1\n",
    "if method is 'randfor':\n",
    "    st = 5\n",
    "else:\n",
    "    st = 1\n",
    "for i in range(0, length, st):\n",
    "    if i == 1:#sliding_window + 1:\n",
    "        model.callbacks.append(EarlyStopping(patience=7, restore_best_weights=True))#5 , baseline=600\n",
    "    if method is not 'randfor' and i > sliding_window + 1 and wfvtraining:\n",
    "        loss = h.history['loss'][-1]\n",
    "        val_loss = min(h.history['val_loss'])\n",
    "        model.epochs = 50\n",
    "    else:\n",
    "        loss = -1\n",
    "        val_loss = -1\n",
    "\n",
    "    sys.stdout.write(\"Walk-Forward Validation %5i/%i: %5d %5d, pred: %6d\\r\" % (i+1, length, loss, val_loss, perr))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    if method in flat:\n",
    "        if method is 'randfor':\n",
    "            # initialize values for lagged power columns\n",
    "            p = []\n",
    "            for l in range(0, timesteps + 0):\n",
    "                p.append([testX[i+j,-l*len(dyn_features + target_features)-2] for j in range(num_sys)])\n",
    "            p = np.array(p)\n",
    "            ps = []\n",
    "            ts = []\n",
    "            ty = []\n",
    "            for f in range(forecast_horizon):\n",
    "                # build input vector for future timestep\n",
    "                t = testX[i+(f*num_sys):i+(f*num_sys)+num_sys]\n",
    "                if t.size > 0:\n",
    "                    for j in range(num_sys):\n",
    "                        for l in range(timesteps-1):\n",
    "                            t[j][-(l*len(dyn_features + target_features))-2] = p[l][j]\n",
    "                            p[l][j] = p[l+1][j]\n",
    "                        t[j][-(timesteps-1)*len(dyn_features + target_features)-2] = p[-1][j]\n",
    "                        t[j][-1] = f\n",
    "                    ts.append(t)\n",
    "                    ty.append(testY[i+(f*num_sys):i+(f*num_sys)+num_sys])\n",
    "\n",
    "                    # make prediction for input new vector\n",
    "                    p[-1] = model.forecast(t)#.item(0)\n",
    "                    ps.append(p[-1].tolist())\n",
    "            ty = np.transpose(np.array(ty), (1,0,2))\n",
    "            ty = ty.reshape(ty.shape[1]* ty.shape[0], ty.shape[2])\n",
    "            ts = np.transpose(np.array(ts),(1,0,2))\n",
    "            ts = ts.reshape(ts.shape[1] * ts.shape[0], ts.shape[2])\n",
    "            \n",
    "            for j in range(num_sys):\n",
    "                predictions.append(pd.DataFrame(pd.DataFrame(ps)[j].rename(columns={1: 0, 2: 0, 3: 0, 4: 0})))\n",
    "\n",
    "            if wfvtraining:\n",
    "                for j in range(num_sys*forecast_horizon):\n",
    "                    trainset.append(ts[j])\n",
    "                    trainy.append(ty[j])\n",
    "                if i - sliding_window >= 0:\n",
    "                    trainset = trainset[-(sliding_window*forecast_horizon):]\n",
    "                    trainy = trainy[-(sliding_window*forecast_horizon):]\n",
    "\n",
    "                if perr > threshold:\n",
    "                    cond = [trainset[-j][-1] == 0 for j in range(len(trainset), 0, -1)]\n",
    "                    #cond = [j//(forecast_horizon * num_sys) - trainset[-j][-1] > 0 for j in range(len(trainset), 0, -1)]\n",
    "                    dfX = np.array(trainset)[cond]\n",
    "                    dfY = np.array(trainy)[cond]\n",
    "                    # train with newly available data\n",
    "                    h = model.learn(dfX, dfY, val_idx=max(int(len(dfY) / val_div), 1))\n",
    "        else:\n",
    "            if recursive:\n",
    "                # initialize values for lagged power columns\n",
    "                p = []\n",
    "                for l in range(0, timesteps + 0):\n",
    "                    p.append(testX[i,-l*len(dyn_features + target_features)-2])\n",
    "\n",
    "                ps = []\n",
    "                ts = []\n",
    "                ty = []\n",
    "                for f in range(forecast_horizon):\n",
    "                    # build input vector for future timestep\n",
    "                    t = testX[i+(f*num_sys)].reshape(1, -1)\n",
    "                    if t.size > 0:\n",
    "                        for l in range(timesteps-1):\n",
    "                            t[0][-(l*len(dyn_features + target_features))-2] = p[l]\n",
    "                            p[l] = p[l+1]\n",
    "                        t[0][-(timesteps-1)*len(dyn_features + target_features)-2] = p[-1]\n",
    "                        t[0][-1] = f\n",
    "                        ts.append(t)\n",
    "                        ty.append(testY[i+(f*num_sys)])\n",
    "\n",
    "                        # make prediction for input new vector\n",
    "                        p[-1] = model.forecast(t).item(0)\n",
    "                        ps.append(p[-1])\n",
    "\n",
    "                perr = np.mean(np.abs(np.concatenate(ty).flatten() - ps))\n",
    "                pred_err.append(perr)\n",
    "                predictions.append(pd.DataFrame(ps))\n",
    "                #print([tsss[0][-1] for tsss in ts])\n",
    "                if wfvtraining:\n",
    "                    trainset += ts\n",
    "                    trainy += ty\n",
    "                    \n",
    "                    #print([tsss[0][-1] for tsss in trainset])\n",
    "                    if i - sliding_window >= 0:\n",
    "                        trainset = trainset[-(sliding_window*forecast_horizon):]\n",
    "                        trainy = trainy[-(sliding_window*forecast_horizon):]\n",
    "                    if perr > threshold and (i+1) % num_sys == 0:###war draußen :(\n",
    "                        #print(24//(forecast_horizon * num_sys))\n",
    "                        #print('#')\n",
    "                        #print([np.array(trainset)[-j][0][-1] for j in range(len(trainset), 0, -1)])\n",
    "                        cond = [trainset[-j][0][-1] == 0 for j in range(len(trainset), 0, -1)]\n",
    "                        #cond = [j//(forecast_horizon * num_sys) - trainset[-j][0][-1] >= 0 for j in range(len(trainset), 0, -1)]\n",
    "                        #cond = [j//forecast_horizon - trainset[-j][0][-1] - ((j+forecast_horizon-1)//forecast_horizon-1) >= 0 for j in range(len(trainset), 0, -1)]\n",
    "                        dfX = np.array(trainset)[cond][:,0]\n",
    "                        dfY = np.array(trainy)[cond][:,0]\n",
    "                        \n",
    "                        #if i >45:\n",
    "                        #    print(asdf)\n",
    "                        #print(dfY)\n",
    "                        #print(dfX[:, :2])\n",
    "                        \n",
    "                        # train with newly available data\n",
    "                        h = model.learn(dfX, dfY, val_idx=max(int(len(dfY) / val_div), 1))\n",
    "            else:\n",
    "                ts = np.array([testX[i]])\n",
    "                ty = np.array([testY[i]])\n",
    "                # make prediction for input new vector\n",
    "                p = pd.DataFrame(model.forecast(ts))\n",
    "                predictions.append(p)\n",
    "                perr = np.mean(np.abs((ty - p).values))\n",
    "                pred_err.append(perr)\n",
    "\n",
    "                trainset.append(ts)\n",
    "                trainy.append(ty)\n",
    "                if i - sliding_window >= 0:\n",
    "                    trainset = trainset[-sliding_window:]\n",
    "                    trainy = trainy[-sliding_window:]\n",
    "\n",
    "                if wfvtraining and perr > threshold:\n",
    "                    w = i - forecast_horizon\n",
    "                    if w > 0:\n",
    "                        dfX = np.concatenate(trainset)\n",
    "                        dfX = dfX[:w+1]\n",
    "                        dfY = np.concatenate(trainy)\n",
    "                        dfY = dfY[:w+1]\n",
    "                        # train with newly available data\n",
    "                        h = model.learn(dfX, dfY, val_idx=max(int(len(dfY) / val_div), 1))\n",
    "    else:\n",
    "        if recursive:\n",
    "            # initialize values for lagged power columns\n",
    "            p = []\n",
    "            for l in range(timesteps):\n",
    "                p.append(testX[i,:][l][-1])\n",
    "            ps = []\n",
    "            ts = []\n",
    "            ty = []\n",
    "            for f in range(forecast_horizon):\n",
    "                # build input vector for future timestep\n",
    "                t = np.array([testX[i+(f*num_sys)]])\n",
    "                if t.size > 0:\n",
    "                    for l in range(timesteps-1):\n",
    "                        t[0][l,-1] = p[l]\n",
    "                        p[l] = p[l+1]\n",
    "                    t[0][-2,-1] = p[-1]\n",
    "                    t[0][:,-2] = f\n",
    "                    ts.append(t)\n",
    "                    ty.append(testY[i+(f*num_sys)])\n",
    "\n",
    "                    # make prediction for input new vector\n",
    "                    p[-1] = model.forecast(t).item(0)\n",
    "                    ps.append(p[-1])\n",
    "\n",
    "            predictions.append(pd.DataFrame(ps))\n",
    "            perr = np.mean(np.abs(np.concatenate(ty).flatten() - ps))\n",
    "            pred_err.append(perr)\n",
    "\n",
    "            if wfvtraining:\n",
    "                trainset += ts\n",
    "                trainy += ty\n",
    "                if i - sliding_window >= 0:\n",
    "                    trainset = trainset[-(sliding_window*forecast_horizon):]\n",
    "                    trainy = trainy[-(sliding_window*forecast_horizon):]\n",
    "                if  perr > threshold and (i+1) % num_sys == 0:\n",
    "                    #cond = [j//forecast_horizon - trainset[-j][0][0,-2] - ((j+forecast_horizon-1)//forecast_horizon-1) >= 0 for j in range(len(trainset), 0, -1)]\n",
    "                    cond = [j//(forecast_horizon * num_sys) - trainset[-j][0][0,-2] >= 0 for j in range(len(trainset), 0, -1)]\n",
    "                    #print(cond)\n",
    "                    #if i > 35:\n",
    "                    #    print(asdf)\n",
    "                    dfX = np.array(trainset)[cond][:,0]\n",
    "                    dfY = np.array(trainy)[cond][:,0]\n",
    "                    # train with newly available data\n",
    "                    h = model.learn(dfX, dfY, val_idx=max(int(len(dfY) / val_div), 1))\n",
    "        else:\n",
    "            ts = np.array([testX[i]])\n",
    "            ty = np.array([testY[i]])\n",
    "            # make prediction for input new vector\n",
    "            p = pd.DataFrame(model.forecast(ts))\n",
    "            predictions.append(p)\n",
    "            perr = np.mean(np.abs((ty - p).values))\n",
    "            pred_err.append(perr)\n",
    "\n",
    "            trainset.append(ts)\n",
    "            trainy.append(ty)\n",
    "            if i - sliding_window >= 0:\n",
    "                trainset = trainset[-sliding_window:]\n",
    "                trainy = trainy[-sliding_window:]\n",
    "                \n",
    "            if wfvtraining and perr > threshold:\n",
    "                w = i - forecast_horizon\n",
    "                if w > 0:\n",
    "                    dfX = np.concatenate(trainset)\n",
    "                    dfX = dfX[:w+1]\n",
    "                    dfY = np.concatenate(trainy)\n",
    "                    dfY = dfY[:w+1]\n",
    "                    # train with newly available data\n",
    "                    h = model.learn(dfX, dfY, val_idx=max(int(len(dfY) / val_div), 1))\n",
    "    \n",
    "prediction = pd.concat(predictions)\n",
    "\n",
    "if method is not 'randfor':\n",
    "    name = './saved_models/trained_t-'+str(timesteps)+'_f'+str(shape[0])+'_e'+str(epochs)+'_b'+str(batch_size)#+'_sys'+str(system)\n",
    "    # serialize model to JSON\n",
    "    model_json = model.model.to_json()\n",
    "    with open(name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.model.save_weights(name + \".h5\")\n",
    "    print(\"\\nSaved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if method is not 'randfor':\n",
    "    if recursive:\n",
    "        plt.plot(np.abs([p.tolist() for p in pred_err]))\n",
    "    else:\n",
    "        plt.plot(np.abs([p for p in pred_err]))\n",
    "    plt.show()\n",
    "\n",
    "prediction = pd.concat(predictions)\n",
    "plt.plot(prediction[0][0].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots = True#False#True\n",
    "comp = 'pvlib'\n",
    "data_list = []\n",
    "for system in range(num_sys):\n",
    "    directory = dir_ + 'system' + str(system + 1) + '/'\n",
    "    set_dir(directory)\n",
    "    meas = testY[system::num_sys]\n",
    "    index = idx[system::num_sys]\n",
    "    pred = pd.concat(predictions[system::num_sys])\n",
    "    pvl = pvlib[system::num_sys]\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(forecast_horizon):\n",
    "        if recursive:\n",
    "            data['+'+str(i+1)+'h-prediction'] = np.pad(pred[0][i].values, (i, 0), mode='constant', constant_values=(np.nan,))[:length]\n",
    "            length = len(data)\n",
    "        else:\n",
    "            data['+'+str(i+1)+'h-prediction'] = np.pad(pred[i].values, (i, forecast_horizon-i-1), mode='constant', constant_values=(np.nan,))\n",
    "\n",
    "    if recursive:\n",
    "        data['measured'] = pd.DataFrame(np.array(meas).reshape([len(meas), len(target_features)])).iloc[:,0]\n",
    "    else:\n",
    "        data['measured'] = pd.DataFrame(np.array(meas)).iloc[:,0].append(pd.DataFrame(np.array(meas)).iloc[-forecast_horizon+1:,-1], ignore_index=True).iloc[:length]\n",
    "    data = data.set_index(pd.MultiIndex.from_tuples(index[:length])).unstack()\n",
    "    data['pvlib'] = pvl[:length]\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp[comp] = data[comp]\n",
    "    tmp['measured'] = data[('measured', 4-system)]#system)]\n",
    "    for i in range(forecast_horizon):\n",
    "        tmp['+'+str(i+1)+'h-prediction'] = data['+'+str(i+1)+'h-prediction']\n",
    "    data = tmp\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    m_col = data['measured']\n",
    "    l_col = data[comp].dropna()\n",
    "\n",
    "    data.describe().to_csv(directory + 'description.csv', encoding='utf-8')\n",
    "    data.corr(method='pearson').to_csv(directory + 'pearson.csv', encoding='utf-8')\n",
    "    data.corr(method='spearman').to_csv(directory + 'spearman.csv', encoding='utf-8')\n",
    "    data.corr(method='kendall').to_csv(directory + 'kendall.csv', encoding='utf-8')\n",
    "    data.to_csv(directory + 'predictions.csv', encoding='utf-8')\n",
    "    \n",
    "    if plots:\n",
    "        print('System: ' + str(system) + '#########################')\n",
    "        for horizon in range(1, forecast_horizon + 1):\n",
    "            name = '+' + str(horizon) + 'h-prediction'\n",
    "            p_col = data[name]\n",
    "\n",
    "            walkForwardDailyLoss(m_col, p_col, l_col, comp, name)\n",
    "            scatter_predictions(m_col, p_col, name)\n",
    "\n",
    "            print('%s test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "            print('%s test RMSE: %.3f' % (comp + ' forecast', math.sqrt(mean_squared_error(m_col, l_col))))\n",
    "            draw_boxplot(m_col, p_col, l_col, comp, name, title='Absolute power prediction error', outliers=False)\n",
    "            #draw_boxplot_monthly(m_col, p_col, l_col, comp, name, 'Monthly power prediction error', 'w', False)\n",
    "\n",
    "            m1, m2 = '2016-07-17 00:00:00', '2016-07-17 23:00:00'\n",
    "            #print('%s nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))\n",
    "            #print('%s nice day RMSE: %.3f' % (comp + ' forecast', math.sqrt(mean_squared_error(m_col[m1:m2], l_col[m1:m2]))))\n",
    "            #draw_boxplot(m_col, p_col, l_col, comp, name, m1, m2, title='Absolute power prediction error', outliers=False)\n",
    "\n",
    "            plot_timeseries(m_col, p_col, l_col, comp, name, end='2015-10-18 00:00:00')\n",
    "            plot_timeseries(m_col, p_col, l_col, comp, name, start='2017-02-02 10:00:00', end='2017-02-09 10:00:00')\n",
    "            plot_timeseries(m_col, p_col, l_col, comp, name, start='2017-12-24 00:00:00')\n",
    "            plot_timeseries(m_col, p_col, l_col, comp, name, start=m1, end=m2)\n",
    "            plot_timeseries(m_col, p_col, l_col, comp, name)\n",
    "            plot_timeseries(m_col, p_col, None, comp, name)\n",
    "\n",
    "            daily_energy_error(m_col, p_col, l_col, comp, name, start='2015-10-13 00:00:00')\n",
    "\n",
    "            draw_histogram(p_col, m_col, name)\n",
    "            print()\n",
    "        plot_error_by_hour_of_day(data, comp, 0, forecast_horizon)#1\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in data_list:\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for i in range(len(data)-(forecast_horizon-1)):\n",
    "        sys.stdout.write(\"%i/%i\\r\" % (i+1, len(data)-(forecast_horizon-1)))\n",
    "        sys.stdout.flush()\n",
    "        forecast = []\n",
    "        for f in range(1, forecast_horizon+1):\n",
    "            forecast.append(data['+'+str(f)+'h-prediction'].iloc[f-1 + i])\n",
    "        a = pd.DataFrame()\n",
    "        a['forecast'] = forecast\n",
    "        a['pvlib'] = data['pvlib'].iloc[i:forecast_horizon+i].values\n",
    "        a['measured'] = data['measured'].iloc[i:forecast_horizon+i].values\n",
    "        l1.append(mean_squared_error(a.measured, a.forecast))\n",
    "        l2.append(mean_squared_error(a.measured, a.pvlib))\n",
    "    print()\n",
    "    print(math.sqrt(pd.DataFrame(l1).mean().values[0]))\n",
    "    print(math.sqrt(pd.DataFrame(l2).mean().values[0]))\n",
    "\n",
    "    plt.plot(np.sqrt(l2))\n",
    "    plt.plot(np.sqrt(l1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 11550#85#1800#1850#110#0#3#436#3272\n",
    "for data in data_list:\n",
    "    print('##############################################')\n",
    "    for i in range(d, d+forecast_horizon):\n",
    "        forecast = []\n",
    "        for f in range(1, forecast_horizon+1):\n",
    "            forecast.append(data['+'+str(f)+'h-prediction'].iloc[f-1 + i])\n",
    "        a = pd.DataFrame()\n",
    "        a['forecast'] = forecast\n",
    "        a['pvlib'] = data['pvlib'].iloc[i:forecast_horizon+i].values\n",
    "        a['measured'] = data['measured'].iloc[i:forecast_horizon+i].values\n",
    "        a.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_dir(dir_)\n",
    "if method is not 'randfor':\n",
    "    #draw_history(history)\n",
    "    draw_history(val_history, True)\n",
    "    print(val_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recursive:\n",
    "    persistence1 = pd.DataFrame()\n",
    "    persistence2 = pd.DataFrame()\n",
    "    persistence1['measured'] = pd.DataFrame(np.array(testY).reshape([len(testY), len(target_features)])).iloc[:,0][::5]\n",
    "    persistence2['measured'] = persistence1['measured']\n",
    "    for f in range(forecast_horizon):\n",
    "        persistence1['+'+str(f+1)+'h-persistence'] = persistence1['measured'].shift(f+1)\n",
    "        persistence2['+'+str(f+1)+'h-persistence'] = persistence2['measured'].shift(24)\n",
    "    persistence1 = persistence1.set_index(pd.MultiIndex.from_tuples(idx[::5])).unstack()\n",
    "    persistence2 = persistence2.set_index(pd.MultiIndex.from_tuples(idx[::5])).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recursive:\n",
    "    m1, m2 = '2016-07-17 00:00:00', '2016-07-17 23:00:00'\n",
    "\n",
    "    print('persistence1:')\n",
    "    persistence1 = persistence1.dropna()\n",
    "    persistence1.corr(method='pearson').to_csv(dir_ + 'pearson_baseline1.csv', encoding='utf-8')\n",
    "    persistence1.to_csv(dir_ + 'predictions_baseline1.csv', encoding='utf-8')\n",
    "    m_col = persistence1['measured']\n",
    "    print()\n",
    "    for f in range(1, forecast_horizon + 1):\n",
    "        name = '+'+str(f)+'h-persistence'\n",
    "        p_col = persistence1[name]\n",
    "        j = int(len(m_col) / 24)\n",
    "        d1 = np.array_split(m_col, j)\n",
    "        d2 = np.array_split(p_col, j)\n",
    "        m_err = pd.DataFrame([math.sqrt(mean_squared_error(d1[i], d2[i])) for i in range(len(d1))]).mean()[0]\n",
    "        print('%s1 test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "        print('daily mean %s1 RMSE: %.3f' % (name, m_err))\n",
    "        print('%s1 nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))\n",
    "\n",
    "    print('\\n\\n\\npersistence2:')\n",
    "    persistence2 = persistence2.dropna()\n",
    "    persistence2.corr(method='pearson').to_csv(dir_ + 'pearson_baseline2.csv', encoding='utf-8')\n",
    "    persistence2.to_csv(dir_ + 'predictions_baseline2.csv', encoding='utf-8')\n",
    "    m_col = persistence2['measured']\n",
    "    print()\n",
    "    for f in range(1, forecast_horizon + 1):\n",
    "        name = '+'+str(f)+'h-persistence'\n",
    "        p_col = persistence2[name]\n",
    "        j = int(len(m_col) / 24)\n",
    "        d1 = np.array_split(m_col, j)\n",
    "        d2 = np.array_split(p_col, j)\n",
    "        m_err = pd.DataFrame([math.sqrt(mean_squared_error(d2[i], d1[i])) for i in range(len(d1))]).mean()[0]\n",
    "        print('%s2 test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "        print('daily mean %s2 RMSE: %.3f' % (name, m_err))\n",
    "        print('%s2 nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(dir_ + 'Experiments/Experiment3/dilated/system1/predictions.csv', skipinitialspace=True).set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '+1h-prediction'\n",
    "p_col = d[name]\n",
    "m_col = d.measured\n",
    "j = int(len(m_col) / 24)\n",
    "d1 = np.array_split(m_col, j)\n",
    "d2 = np.array_split(p_col, j)\n",
    "m_err = pd.DataFrame([math.sqrt(mean_squared_error(d2[i], d1[i])) for i in range(len(d1))]).mean()[0]\n",
    "\n",
    "print('%s test RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col, p_col))))\n",
    "print('daily mean %s RMSE: %.3f' % (name, m_err))\n",
    "print('%s nice day RMSE: %.3f' % (name, math.sqrt(mean_squared_error(m_col[m1:m2], p_col[m1:m2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ['randfor', 'mlp', 'lstm', 'dilated']\n",
    "df = pd.DataFrame()\n",
    "for m in ms:\n",
    "    mss = ['Random Forest', 'MLP', 'LSTM', 'Dilated CNN']\n",
    "    d = pd.read_csv(dir_ + 'Experiments/Experiment1/' + m + '/system1/pearson.csv', skipinitialspace=True)\n",
    "    df[mss[ms.index(m)]] = d.measured[2:]\n",
    "df['PVLIB'] = d.measured[0]\n",
    "df['24h-persistence'] = 0.802140655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.set_index(np.arange(1, 25))\n",
    "df.plot(figsize=(10,10))\n",
    "plt.title('Pearson Correlation with increasing forecast horizon')\n",
    "plt.xlabel('Hours ahead')\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.grid(True)\n",
    "plt.savefig(dir_ + 'Corr_Exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_jobs=-1, criterion='mse', random_state=1)\n",
    "\n",
    "# Try different numbers of n_estimators - this will take a minute or so\n",
    "estimators = np.arange(10, 200, 5)\n",
    "scores = []\n",
    "for n in estimators:\n",
    "    model.set_params(n_estimators=n)\n",
    "    model.fit(testX[:120], testY[:120].ravel())\n",
    "    scores.append(model.score(testX[:120], testY[:120]))\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_title(\"Effect of number of estimators\")\n",
    "ax.set_xlabel(\"Number of estimators\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.plot(estimators, scores)\n",
    "plt.grid(True)\n",
    "plt.savefig(dir_ + 'n_estimator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=75, n_jobs=-1, criterion='mse', random_state=1)\n",
    "regr = model.fit(testX, testY.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame(regr.feature_importances_, index=dataset.columns.values[:-1]).sort_values(by=[0], ascending=False).rename(columns={0: 'importance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp[:20].plot.barh()\n",
    "plt.xscale('log')\n",
    "plt.savefig(dir_ + 'importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(dir_ + 'pearson_correlations.csv', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(testX, columns=dataset.columns.values[:-1])#.corr('pearson')\n",
    "df['power'] = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = df.corr('pearson').power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cs.abs().sort_values(ascending=False)).rename(columns={'power': 'correlation'})[1:21].plot.barh()\n",
    "plt.xscale('linear')\n",
    "plt.savefig(dir_ + 'corrs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cs.abs().sort_values(ascending=False)).rename(columns={'power': 'correlation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
